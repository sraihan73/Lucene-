using namespace std;

#include "TestICUTokenizer.h"
#include "../../../../../../../../../../core/src/java/org/apache/lucene/analysis/Tokenizer.h"
#include "../../../../../../../java/org/apache/lucene/analysis/icu/segmentation/DefaultICUTokenizerConfig.h"
#include "../../../../../../../java/org/apache/lucene/analysis/icu/segmentation/ICUTokenizer.h"
#include "../../../../../../../java/org/apache/lucene/analysis/icu/tokenattributes/ScriptAttribute.h"

namespace org::apache::lucene::analysis::icu::segmentation
{
using Analyzer = org::apache::lucene::analysis::Analyzer;
using BaseTokenStreamTestCase =
    org::apache::lucene::analysis::BaseTokenStreamTestCase;
using TokenStream = org::apache::lucene::analysis::TokenStream;
using Tokenizer = org::apache::lucene::analysis::Tokenizer;
using ScriptAttribute =
    org::apache::lucene::analysis::icu::tokenattributes::ScriptAttribute;
using com::ibm::icu::lang::UScript;

void TestICUTokenizer::testHugeDoc() 
{
  shared_ptr<StringBuilder> sb = make_shared<StringBuilder>();
  std::deque<wchar_t> whitespace(4094);
  Arrays::fill(whitespace, L' ');
  sb->append(whitespace);
  sb->append(L"testing 1234");
  wstring input = sb->toString();
  shared_ptr<ICUTokenizer> tokenizer = make_shared<ICUTokenizer>(
      newAttributeFactory(),
      make_shared<DefaultICUTokenizerConfig>(false, true));
  tokenizer->setReader(make_shared<StringReader>(input));
  assertTokenStreamContents(tokenizer,
                            std::deque<wstring>{L"testing", L"1234"});
}

void TestICUTokenizer::testHugeTerm2() 
{
  shared_ptr<StringBuilder> sb = make_shared<StringBuilder>();
  for (int i = 0; i < 40960; i++) {
    sb->append(L'a');
  }
  wstring input = sb->toString();
  shared_ptr<ICUTokenizer> tokenizer = make_shared<ICUTokenizer>(
      newAttributeFactory(),
      make_shared<DefaultICUTokenizerConfig>(false, true));
  tokenizer->setReader(make_shared<StringReader>(input));
  std::deque<wchar_t> token(4096);
  Arrays::fill(token, L'a');
  wstring expectedToken = wstring(token);
  std::deque<wstring> expected = {expectedToken, expectedToken, expectedToken,
                                   expectedToken, expectedToken, expectedToken,
                                   expectedToken, expectedToken, expectedToken,
                                   expectedToken};
  assertTokenStreamContents(tokenizer, expected);
}

void TestICUTokenizer::setUp() 
{
  BaseTokenStreamTestCase::setUp();
  a = make_shared<AnalyzerAnonymousInnerClass>(shared_from_this());
}

TestICUTokenizer::AnalyzerAnonymousInnerClass::AnalyzerAnonymousInnerClass(
    shared_ptr<TestICUTokenizer> outerInstance)
{
  this->outerInstance = outerInstance;
}

shared_ptr<Analyzer::TokenStreamComponents>
TestICUTokenizer::AnalyzerAnonymousInnerClass::createComponents(
    const wstring &fieldName)
{
  shared_ptr<Tokenizer> tokenizer = make_shared<ICUTokenizer>(
      BaseTokenStreamTestCase::newAttributeFactory(),
      make_shared<DefaultICUTokenizerConfig>(false, true));
  return make_shared<Analyzer::TokenStreamComponents>(tokenizer);
}

void TestICUTokenizer::tearDown() 
{
  delete a;
  BaseTokenStreamTestCase::tearDown();
}

void TestICUTokenizer::testArmenian() 
{
  assertAnalyzesTo(
      a,
      L"ÕÕ«Ö„Õ«ÕºÕ¥Õ¤Õ«Õ¡ÕµÕ« 13 Õ´Õ«Õ¬Õ«Õ¸Õ¶ Õ°Õ¸Õ¤Õ¾Õ¡Õ®Õ¶Õ¥Ö€Õ¨ (4,600` Õ°Õ¡ÕµÕ¥Ö€Õ¥Õ¶ Õ¾Õ«Ö„Õ«ÕºÕ¥Õ¤Õ«Õ¡ÕµÕ¸Ö‚Õ´) Õ£Ö€Õ¾Õ¥Õ¬ "
      L"Õ¥Õ¶ Õ¯Õ¡Õ´Õ¡Õ¾Õ¸Ö€Õ¶Õ¥Ö€Õ« Õ¯Õ¸Õ²Õ´Õ«Ö Õ¸Ö‚ Õ°Õ¡Õ´Õ¡Ö€ÕµÕ¡ Õ¢Õ¸Õ¬Õ¸Ö€ Õ°Õ¸Õ¤Õ¾Õ¡Õ®Õ¶Õ¥Ö€Õ¨ Õ¯Õ¡Ö€Õ¸Õ² Õ§ Õ­Õ´Õ¢Õ¡Õ£Ö€Õ¥Õ¬ "
      L"ÖÕ¡Õ¶Õ¯Õ¡Ö Õ´Õ¡Ö€Õ¤ Õ¸Õ¾ Õ¯Õ¡Ö€Õ¸Õ² Õ§ Õ¢Õ¡ÖÕ¥Õ¬ ÕÕ«Ö„Õ«ÕºÕ¥Õ¤Õ«Õ¡ÕµÕ« Õ¯Õ¡ÕµÖ„Õ¨Ö‰",
      std::deque<wstring>{L"ÕÕ«Ö„Õ«ÕºÕ¥Õ¤Õ«Õ¡ÕµÕ«",   L"13",     L"Õ´Õ«Õ¬Õ«Õ¸Õ¶",
                           L"Õ°Õ¸Õ¤Õ¾Õ¡Õ®Õ¶Õ¥Ö€Õ¨",    L"4,600",  L"Õ°Õ¡ÕµÕ¥Ö€Õ¥Õ¶",
                           L"Õ¾Õ«Ö„Õ«ÕºÕ¥Õ¤Õ«Õ¡ÕµÕ¸Ö‚Õ´", L"Õ£Ö€Õ¾Õ¥Õ¬",  L"Õ¥Õ¶",
                           L"Õ¯Õ¡Õ´Õ¡Õ¾Õ¸Ö€Õ¶Õ¥Ö€Õ«",   L"Õ¯Õ¸Õ²Õ´Õ«Ö", L"Õ¸Ö‚",
                           L"Õ°Õ¡Õ´Õ¡Ö€ÕµÕ¡",       L"Õ¢Õ¸Õ¬Õ¸Ö€",  L"Õ°Õ¸Õ¤Õ¾Õ¡Õ®Õ¶Õ¥Ö€Õ¨",
                           L"Õ¯Õ¡Ö€Õ¸Õ²",         L"Õ§",      L"Õ­Õ´Õ¢Õ¡Õ£Ö€Õ¥Õ¬",
                           L"ÖÕ¡Õ¶Õ¯Õ¡Ö",        L"Õ´Õ¡Ö€Õ¤",   L"Õ¸Õ¾",
                           L"Õ¯Õ¡Ö€Õ¸Õ²",         L"Õ§",      L"Õ¢Õ¡ÖÕ¥Õ¬",
                           L"ÕÕ«Ö„Õ«ÕºÕ¥Õ¤Õ«Õ¡ÕµÕ«",   L"Õ¯Õ¡ÕµÖ„Õ¨"});
}

void TestICUTokenizer::testAmharic() 
{
  assertAnalyzesTo(
      a, L"á‹ŠáŠªá”á‹µá‹« á‹¨á‰£áˆˆ á‰¥á‹™ á‰‹áŠ•á‰‹ á‹¨á‰°áˆŸáˆ‹ á‰µáŠ­áŠ­áˆˆáŠ›áŠ“ áŠáŒ» áˆ˜á‹áŒˆá‰  á‹•á‹á‰€á‰µ (áŠ¢áŠ•áˆ³á‹­áŠ­áˆá’á‹²á‹«) áŠá‹á¢ áˆ›áŠ•áŠ›á‹áˆ",
      std::deque<wstring>{L"á‹ŠáŠªá”á‹µá‹«", L"á‹¨á‰£áˆˆ", L"á‰¥á‹™", L"á‰‹áŠ•á‰‹", L"á‹¨á‰°áˆŸáˆ‹", L"á‰µáŠ­áŠ­áˆˆáŠ›áŠ“",
                           L"áŠáŒ»", L"áˆ˜á‹áŒˆá‰ ", L"á‹•á‹á‰€á‰µ", L"áŠ¢áŠ•áˆ³á‹­áŠ­áˆá’á‹²á‹«", L"áŠá‹",
                           L"áˆ›áŠ•áŠ›á‹áˆ"});
}

void TestICUTokenizer::testArabic() 
{
  assertAnalyzesTo(a,
                   L"Ø§Ù„ÙÙŠÙ„Ù… Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ÙŠ Ø§Ù„Ø£ÙˆÙ„ Ø¹Ù† ÙˆÙŠÙƒÙŠØ¨ÙŠØ¯ÙŠØ§ ÙŠØ³Ù…Ù‰ \"Ø§Ù„Ø­Ù‚ÙŠÙ‚Ø© "
                   L"Ø¨Ø§Ù„Ø£Ø±Ù‚Ø§Ù…: Ù‚ØµØ© ÙˆÙŠÙƒÙŠØ¨ÙŠØ¯ÙŠØ§\" (Ø¨Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©: Truth in Numbers: "
                   L"The Wikipedia Story)ØŒ Ø³ÙŠØªÙ… Ø¥Ø·Ù„Ø§Ù‚Ù‡ ÙÙŠ 2008.",
                   std::deque<wstring>{L"Ø§Ù„ÙÙŠÙ„Ù…",    L"Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ÙŠ",    L"Ø§Ù„Ø£ÙˆÙ„",
                                        L"Ø¹Ù†",        L"ÙˆÙŠÙƒÙŠØ¨ÙŠØ¯ÙŠØ§",   L"ÙŠØ³Ù…Ù‰",
                                        L"Ø§Ù„Ø­Ù‚ÙŠÙ‚Ø©",   L"Ø¨Ø§Ù„Ø£Ø±Ù‚Ø§Ù…",    L"Ù‚ØµØ©",
                                        L"ÙˆÙŠÙƒÙŠØ¨ÙŠØ¯ÙŠØ§", L"Ø¨Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©", L"Truth",
                                        L"in",        L"Numbers",     L"The",
                                        L"Wikipedia", L"Story",       L"Ø³ÙŠØªÙ…",
                                        L"Ø¥Ø·Ù„Ø§Ù‚Ù‡",    L"ÙÙŠ",          L"2008"});
}

void TestICUTokenizer::testAramaic() 
{
  assertAnalyzesTo(a,
                   L"Ü˜ÜÜ©ÜÜ¦Ü•ÜÜ (ÜÜ¢Ü“Ü ÜÜ: Wikipedia) Ü—Ü˜ ÜÜÜ¢Ü£Ü©Ü Ü˜Ü¦Ü•ÜÜ ÜšÜÜªÜ¬Ü Ü•ÜÜ¢Ü›ÜªÜ¢Ü› "
                   L"Ü’Ü Ü«Ü¢ÌˆÜ Ü£Ü“ÜÜÌˆÜÜ‚ Ü«Ü¡Ü— ÜÜ¬Ü Ü¡Ü¢ Ü¡ÌˆÜ Ü¬Ü Ü•\"Ü˜ÜÜ©Ü\" Ü˜\"ÜÜÜ¢Ü£Ü©Ü Ü˜Ü¦Ü•ÜÜ\"Ü€",
                   std::deque<wstring>{L"Ü˜ÜÜ©ÜÜ¦Ü•ÜÜ", L"ÜÜ¢Ü“Ü ÜÜ", L"Wikipedia",
                                        L"Ü—Ü˜", L"ÜÜÜ¢Ü£Ü©Ü Ü˜Ü¦Ü•ÜÜ", L"ÜšÜÜªÜ¬Ü",
                                        L"Ü•ÜÜ¢Ü›ÜªÜ¢Ü›", L"Ü’Ü Ü«Ü¢ÌˆÜ", L"Ü£Ü“ÜÜÌˆÜ", L"Ü«Ü¡Ü—",
                                        L"ÜÜ¬Ü", L"Ü¡Ü¢", L"Ü¡ÌˆÜ Ü¬Ü", L"Ü•", L"Ü˜ÜÜ©Ü",
                                        L"Ü˜", L"ÜÜÜ¢Ü£Ü©Ü Ü˜Ü¦Ü•ÜÜ"});
}

void TestICUTokenizer::testBengali() 
{
  assertAnalyzesTo(a,
                   L"à¦à¦‡ à¦¬à¦¿à¦¶à§à¦¬à¦•à§‹à¦· à¦ªà¦°à¦¿à¦šà¦¾à¦²à¦¨à¦¾ à¦•à¦°à§‡ à¦‰à¦‡à¦•à¦¿à¦®à¦¿à¦¡à¦¿à¦¯à¦¼à¦¾ à¦«à¦¾à¦‰à¦¨à§à¦¡à§‡à¦¶à¦¨ (à¦à¦•à¦Ÿà¦¿ à¦…à¦²à¦¾à¦­à¦œà¦¨à¦• "
                   L"à¦¸à¦‚à¦¸à§à¦¥à¦¾)à¥¤ à¦‰à¦‡à¦•à¦¿à¦ªà¦¿à¦¡à¦¿à¦¯à¦¼à¦¾à¦° à¦¶à§à¦°à§ à§§à§« à¦œà¦¾à¦¨à§à¦¯à¦¼à¦¾à¦°à¦¿, à§¨à§¦à§¦à§§ à¦¸à¦¾à¦²à§‡à¥¤ à¦à¦–à¦¨ à¦ªà¦°à§à¦¯à¦¨à§à¦¤ "
                   L"à§¨à§¦à§¦à¦Ÿà¦¿à¦°à¦“ à¦¬à§‡à¦¶à§€ à¦­à¦¾à¦·à¦¾à¦¯à¦¼ à¦‰à¦‡à¦•à¦¿à¦ªà¦¿à¦¡à¦¿à¦¯à¦¼à¦¾ à¦°à¦¯à¦¼à§‡à¦›à§‡à¥¤",
                   std::deque<wstring>{
                       L"à¦à¦‡",         L"à¦¬à¦¿à¦¶à§à¦¬à¦•à§‹à¦·",     L"à¦ªà¦°à¦¿à¦šà¦¾à¦²à¦¨à¦¾", L"à¦•à¦°à§‡",
                       L"à¦‰à¦‡à¦•à¦¿à¦®à¦¿à¦¡à¦¿à¦¯à¦¼à¦¾", L"à¦«à¦¾à¦‰à¦¨à§à¦¡à§‡à¦¶à¦¨",    L"à¦à¦•à¦Ÿà¦¿",     L"à¦…à¦²à¦¾à¦­à¦œà¦¨à¦•",
                       L"à¦¸à¦‚à¦¸à§à¦¥à¦¾",      L"à¦‰à¦‡à¦•à¦¿à¦ªà¦¿à¦¡à¦¿à¦¯à¦¼à¦¾à¦°", L"à¦¶à§à¦°à§",       L"à§§à§«",
                       L"à¦œà¦¾à¦¨à§à¦¯à¦¼à¦¾à¦°à¦¿",    L"à§¨à§¦à§¦à§§",        L"à¦¸à¦¾à¦²à§‡",     L"à¦à¦–à¦¨",
                       L"à¦ªà¦°à§à¦¯à¦¨à§à¦¤",      L"à§¨à§¦à§¦à¦Ÿà¦¿à¦°à¦“",     L"à¦¬à§‡à¦¶à§€",     L"à¦­à¦¾à¦·à¦¾à¦¯à¦¼",
                       L"à¦‰à¦‡à¦•à¦¿à¦ªà¦¿à¦¡à¦¿à¦¯à¦¼à¦¾", L"à¦°à¦¯à¦¼à§‡à¦›à§‡"});
}

void TestICUTokenizer::testFarsi() 
{
  assertAnalyzesTo(a,
                   L"ÙˆÛŒÚ©ÛŒ Ù¾Ø¯ÛŒØ§ÛŒ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ Ø¯Ø± ØªØ§Ø±ÛŒØ® Û²Ûµ Ø¯ÛŒ Û±Û³Û·Û¹ Ø¨Ù‡ ØµÙˆØ±Øª Ù…Ú©Ù…Ù„ÛŒ Ø¨Ø±Ø§ÛŒ "
                   L"Ø¯Ø§Ù†Ø´Ù†Ø§Ù…Ù‡Ù” ØªØ®ØµØµÛŒ Ù†ÙˆÙ¾Ø¯ÛŒØ§ Ù†ÙˆØ´ØªÙ‡ Ø´Ø¯.",
                   std::deque<wstring>{L"ÙˆÛŒÚ©ÛŒ", L"Ù¾Ø¯ÛŒØ§ÛŒ", L"Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ", L"Ø¯Ø±",
                                        L"ØªØ§Ø±ÛŒØ®", L"Û²Ûµ", L"Ø¯ÛŒ", L"Û±Û³Û·Û¹", L"Ø¨Ù‡",
                                        L"ØµÙˆØ±Øª", L"Ù…Ú©Ù…Ù„ÛŒ", L"Ø¨Ø±Ø§ÛŒ", L"Ø¯Ø§Ù†Ø´Ù†Ø§Ù…Ù‡Ù”",
                                        L"ØªØ®ØµØµÛŒ", L"Ù†ÙˆÙ¾Ø¯ÛŒØ§", L"Ù†ÙˆØ´ØªÙ‡", L"Ø´Ø¯"});
}

void TestICUTokenizer::testGreek() 
{
  assertAnalyzesTo(
      a,
      L"Î“ÏÎ¬Ï†ÎµÏ„Î±Î¹ ÏƒÎµ ÏƒÏ…Î½ÎµÏÎ³Î±ÏƒÎ¯Î± Î±Ï€ÏŒ ÎµÎ¸ÎµÎ»Î¿Î½Ï„Î­Ï‚ Î¼Îµ Ï„Î¿ Î»Î¿Î³Î¹ÏƒÎ¼Î¹ÎºÏŒ wiki, ÎºÎ¬Ï„Î¹ Ï€Î¿Ï… "
      L"ÏƒÎ·Î¼Î±Î¯Î½ÎµÎ¹ ÏŒÏ„Î¹ Î¬ÏÎ¸ÏÎ± Î¼Ï€Î¿ÏÎµÎ¯ Î½Î± Ï€ÏÎ¿ÏƒÏ„ÎµÎ¸Î¿ÏÎ½ Î® Î½Î± Î±Î»Î»Î¬Î¾Î¿Ï…Î½ Î±Ï€ÏŒ Ï„Î¿Î½ ÎºÎ±Î¸Î­Î½Î±.",
      std::deque<wstring>{L"Î“ÏÎ¬Ï†ÎµÏ„Î±Î¹",   L"ÏƒÎµ",    L"ÏƒÏ…Î½ÎµÏÎ³Î±ÏƒÎ¯Î±", L"Î±Ï€ÏŒ",
                           L"ÎµÎ¸ÎµÎ»Î¿Î½Ï„Î­Ï‚",  L"Î¼Îµ",    L"Ï„Î¿",         L"Î»Î¿Î³Î¹ÏƒÎ¼Î¹ÎºÏŒ",
                           L"wiki",       L"ÎºÎ¬Ï„Î¹",  L"Ï€Î¿Ï…",        L"ÏƒÎ·Î¼Î±Î¯Î½ÎµÎ¹",
                           L"ÏŒÏ„Î¹",        L"Î¬ÏÎ¸ÏÎ±", L"Î¼Ï€Î¿ÏÎµÎ¯",     L"Î½Î±",
                           L"Ï€ÏÎ¿ÏƒÏ„ÎµÎ¸Î¿ÏÎ½", L"Î®",     L"Î½Î±",         L"Î±Î»Î»Î¬Î¾Î¿Ï…Î½",
                           L"Î±Ï€ÏŒ",        L"Ï„Î¿Î½",   L"ÎºÎ±Î¸Î­Î½Î±"});
}

void TestICUTokenizer::testKhmer() 
{
  assertAnalyzesTo(
      a, L"á•áŸ’á‘áŸ‡áŸáŸ’á€á¹á˜áŸáŸ’á€áŸƒá”á¸á”á½á“ááŸ’á“á„á“áŸáŸ‡",
      std::deque<wstring>{L"á•áŸ’á‘áŸ‡", L"áŸáŸ’á€á¹á˜áŸáŸ’á€áŸƒ", L"á”á¸", L"á”á½á“", L"ááŸ’á“á„", L"á“áŸáŸ‡"});
}

void TestICUTokenizer::testLao() 
{
  assertAnalyzesTo(a, L"àºàº§à»ˆàº²àº”àº­àº", std::deque<wstring>{L"àºàº§à»ˆàº²", L"àº”àº­àº"});
  assertAnalyzesTo(a, L"àºàº²àºªàº²àº¥àº²àº§", std::deque<wstring>{L"àºàº²àºªàº²", L"àº¥àº²àº§"},
                   std::deque<wstring>{L"<ALPHANUM>", L"<ALPHANUM>"});
}

void TestICUTokenizer::testMyanmar() 
{
  assertAnalyzesTo(a, L"á€á€€á€ºá€á€„á€ºá€œá€¾á€¯á€•á€ºá€›á€¾á€¬á€¸á€…á€±á€•á€¼á€®á€¸",
                   std::deque<wstring>{L"á€á€€á€ºá€á€„á€º", L"á€œá€¾á€¯á€•á€ºá€›á€¾á€¬á€¸", L"á€…á€±", L"á€•á€¼á€®á€¸"});
}

void TestICUTokenizer::testThai() 
{
  assertAnalyzesTo(a, L"à¸à¸²à¸£à¸—à¸µà¹ˆà¹„à¸”à¹‰à¸•à¹‰à¸­à¸‡à¹à¸ªà¸”à¸‡à¸§à¹ˆà¸²à¸‡à¸²à¸™à¸”à¸µ. à¹à¸¥à¹‰à¸§à¹€à¸˜à¸­à¸ˆà¸°à¹„à¸›à¹„à¸«à¸™? à¹‘à¹’à¹“à¹”",
                   std::deque<wstring>{L"à¸à¸²à¸£", L"à¸—à¸µà¹ˆ", L"à¹„à¸”à¹‰", L"à¸•à¹‰à¸­à¸‡", L"à¹à¸ªà¸”à¸‡",
                                        L"à¸§à¹ˆà¸²", L"à¸‡à¸²à¸™", L"à¸”à¸µ", L"à¹à¸¥à¹‰à¸§", L"à¹€à¸˜à¸­",
                                        L"à¸ˆà¸°", L"à¹„à¸›", L"à¹„à¸«à¸™", L"à¹‘à¹’à¹“à¹”"});
}

void TestICUTokenizer::testTibetan() 
{
  assertAnalyzesTo(
      a, L"à½¦à¾£à½¼à½“à¼‹à½˜à½›à½¼à½‘à¼‹à½‘à½„à¼‹à½£à½¦à¼‹à½ à½‘à½²à½¦à¼‹à½–à½¼à½‘à¼‹à½¡à½²à½‚à¼‹à½˜à½²à¼‹à½‰à½˜à½¦à¼‹à½‚à½¼à½„à¼‹à½ à½•à½ºà½£à¼‹à½‘à½´à¼‹à½‚à½à½¼à½„à¼‹à½–à½¢à¼‹à½§à¼‹à½…à½„à¼‹à½‘à½‚à½ºà¼‹à½˜à½šà½“à¼‹à½˜à½†à½²à½¦à¼‹à½¦à½¼à¼ à¼",
      std::deque<wstring>{L"à½¦à¾£à½¼à½“", L"à½˜à½›à½¼à½‘", L"à½‘à½„", L"à½£à½¦",  L"à½ à½‘à½²à½¦", L"à½–à½¼à½‘",  L"à½¡à½²à½‚",
                           L"à½˜à½²",  L"à½‰à½˜à½¦", L"à½‚à½¼à½„", L"à½ à½•à½ºà½£", L"à½‘à½´",   L"à½‚à½à½¼à½„", L"à½–à½¢",
                           L"à½§",  L"à½…à½„",  L"à½‘à½‚à½º", L"à½˜à½šà½“", L"à½˜à½†à½²à½¦", L"à½¦à½¼"});
}

void TestICUTokenizer::testChinese() 
{
  assertAnalyzesTo(a, L"æˆ‘æ˜¯ä¸­å›½äººã€‚ ï¼‘ï¼’ï¼“ï¼” ï¼´ï½…ï½“ï½”ï½“ ",
                   std::deque<wstring>{L"æˆ‘", L"æ˜¯", L"ä¸­", L"å›½", L"äºº",
                                        L"ï¼‘ï¼’ï¼“ï¼”", L"ï¼´ï½…ï½“ï½”ï½“"});
}

void TestICUTokenizer::testHebrew() 
{
  assertAnalyzesTo(a, L"×“× ×§× ×¨ ×ª×§×£ ××ª ×”×“×•\"×—",
                   std::deque<wstring>{L"×“× ×§× ×¨", L"×ª×§×£", L"××ª", L"×”×“×•\"×—"});
  assertAnalyzesTo(a, L"×—×‘×¨×ª ×‘×ª ×©×œ ××•×“×™'×¡",
                   std::deque<wstring>{L"×—×‘×¨×ª", L"×‘×ª", L"×©×œ", L"××•×“×™'×¡"});
}

void TestICUTokenizer::testEmpty() 
{
  assertAnalyzesTo(a, L"", std::deque<wstring>());
  assertAnalyzesTo(a, L".", std::deque<wstring>());
  assertAnalyzesTo(a, L" ", std::deque<wstring>());
}

void TestICUTokenizer::testLUCENE1545() 
{
  /*
   * Standard analyzer does not correctly tokenize combining character U+0364
   * COMBINING LATIN SMALL LETTRE E. The word "moÍ¤chte" is incorrectly tokenized
   * into "mo" "chte", the combining character is lost. Expected result is only
   * on token "moÍ¤chte".
   */
  assertAnalyzesTo(a, L"moÍ¤chte", std::deque<wstring>{L"moÍ¤chte"});
}

void TestICUTokenizer::testAlphanumericSA() 
{
  // alphanumeric tokens
  assertAnalyzesTo(a, L"B2B", std::deque<wstring>{L"B2B"});
  assertAnalyzesTo(a, L"2B", std::deque<wstring>{L"2B"});
}

void TestICUTokenizer::testDelimitersSA() 
{
  // other delimiters: "-", "/", ","
  assertAnalyzesTo(a, L"some-dashed-phrase",
                   std::deque<wstring>{L"some", L"dashed", L"phrase"});
  assertAnalyzesTo(a, L"dogs,chase,cats",
                   std::deque<wstring>{L"dogs", L"chase", L"cats"});
  assertAnalyzesTo(a, L"ac/dc", std::deque<wstring>{L"ac", L"dc"});
}

void TestICUTokenizer::testApostrophesSA() 
{
  // internal apostrophes: O'Reilly, you're, O'Reilly's
  assertAnalyzesTo(a, L"O'Reilly", std::deque<wstring>{L"O'Reilly"});
  assertAnalyzesTo(a, L"you're", std::deque<wstring>{L"you're"});
  assertAnalyzesTo(a, L"she's", std::deque<wstring>{L"she's"});
  assertAnalyzesTo(a, L"Jim's", std::deque<wstring>{L"Jim's"});
  assertAnalyzesTo(a, L"don't", std::deque<wstring>{L"don't"});
  assertAnalyzesTo(a, L"O'Reilly's", std::deque<wstring>{L"O'Reilly's"});
}

void TestICUTokenizer::testNumericSA() 
{
  // floating point, serial, model numbers, ip addresses, etc.
  // every other segment must have at least one digit
  assertAnalyzesTo(a, L"21.35", std::deque<wstring>{L"21.35"});
  assertAnalyzesTo(a, L"R2D2 C3PO", std::deque<wstring>{L"R2D2", L"C3PO"});
  assertAnalyzesTo(a, L"216.239.63.104",
                   std::deque<wstring>{L"216.239.63.104"});
  assertAnalyzesTo(a, L"216.239.63.104",
                   std::deque<wstring>{L"216.239.63.104"});
}

void TestICUTokenizer::testTextWithNumbersSA() 
{
  // numbers
  assertAnalyzesTo(a, L"David has 5000 bones",
                   std::deque<wstring>{L"David", L"has", L"5000", L"bones"});
}

void TestICUTokenizer::testVariousTextSA() 
{
  // various
  assertAnalyzesTo(
      a, L"C embedded developers wanted",
      std::deque<wstring>{L"C", L"embedded", L"developers", L"wanted"});
  assertAnalyzesTo(a, L"foo bar FOO BAR",
                   std::deque<wstring>{L"foo", L"bar", L"FOO", L"BAR"});
  assertAnalyzesTo(a, L"foo      bar .  FOO <> BAR",
                   std::deque<wstring>{L"foo", L"bar", L"FOO", L"BAR"});
  assertAnalyzesTo(a, L"\"QUOTED\" word",
                   std::deque<wstring>{L"QUOTED", L"word"});
}

void TestICUTokenizer::testKoreanSA() 
{
  // Korean words
  assertAnalyzesTo(a, L"ì•ˆë…•í•˜ì„¸ìš” í•œê¸€ì…ë‹ˆë‹¤",
                   std::deque<wstring>{L"ì•ˆë…•í•˜ì„¸ìš”", L"í•œê¸€ì…ë‹ˆë‹¤"});
}

void TestICUTokenizer::testReusableTokenStream() 
{
  assertAnalyzesTo(
      a, L"à½¦à¾£à½¼à½“à¼‹à½˜à½›à½¼à½‘à¼‹à½‘à½„à¼‹à½£à½¦à¼‹à½ à½‘à½²à½¦à¼‹à½–à½¼à½‘à¼‹à½¡à½²à½‚à¼‹à½˜à½²à¼‹à½‰à½˜à½¦à¼‹à½‚à½¼à½„à¼‹à½ à½•à½ºà½£à¼‹à½‘à½´à¼‹à½‚à½à½¼à½„à¼‹à½–à½¢à¼‹à½§à¼‹à½…à½„à¼‹à½‘à½‚à½ºà¼‹à½˜à½šà½“à¼‹à½˜à½†à½²à½¦à¼‹à½¦à½¼à¼ à¼",
      std::deque<wstring>{L"à½¦à¾£à½¼à½“", L"à½˜à½›à½¼à½‘", L"à½‘à½„", L"à½£à½¦",  L"à½ à½‘à½²à½¦", L"à½–à½¼à½‘",  L"à½¡à½²à½‚",
                           L"à½˜à½²",  L"à½‰à½˜à½¦", L"à½‚à½¼à½„", L"à½ à½•à½ºà½£", L"à½‘à½´",   L"à½‚à½à½¼à½„", L"à½–à½¢",
                           L"à½§",  L"à½…à½„",  L"à½‘à½‚à½º", L"à½˜à½šà½“", L"à½˜à½†à½²à½¦", L"à½¦à½¼"});
}

void TestICUTokenizer::testOffsets() 
{
  assertAnalyzesTo(a, L"David has 5000 bones",
                   std::deque<wstring>{L"David", L"has", L"5000", L"bones"},
                   std::deque<int>{0, 6, 10, 15},
                   std::deque<int>{5, 9, 14, 20});
}

void TestICUTokenizer::testTypes() 
{
  assertAnalyzesTo(a, L"David has 5000 bones",
                   std::deque<wstring>{L"David", L"has", L"5000", L"bones"},
                   std::deque<wstring>{L"<ALPHANUM>", L"<ALPHANUM>", L"<NUM>",
                                        L"<ALPHANUM>"});
}

void TestICUTokenizer::testKorean() 
{
  BaseTokenStreamTestCase::assertAnalyzesTo(a, L"í›ˆë¯¼ì •ìŒ",
                                            std::deque<wstring>{L"í›ˆë¯¼ì •ìŒ"},
                                            std::deque<wstring>{L"<HANGUL>"});
}

void TestICUTokenizer::testJapanese() 
{
  BaseTokenStreamTestCase::assertAnalyzesTo(
      a, L"ä»®åé£ã„ ã‚«ã‚¿ã‚«ãƒŠ",
      std::deque<wstring>{L"ä»®", L"å", L"é£", L"ã„", L"ã‚«ã‚¿ã‚«ãƒŠ"},
      std::deque<wstring>{L"<IDEOGRAPHIC>", L"<IDEOGRAPHIC>", L"<IDEOGRAPHIC>",
                           L"<HIRAGANA>", L"<KATAKANA>"});
}

void TestICUTokenizer::testEmoji() 
{
  BaseTokenStreamTestCase::assertAnalyzesTo(
      a, L"ğŸ’© ğŸ’©ğŸ’©", std::deque<wstring>{L"ğŸ’©", L"ğŸ’©", L"ğŸ’©"},
      std::deque<wstring>{L"<EMOJI>", L"<EMOJI>", L"<EMOJI>"});
}

void TestICUTokenizer::testEmojiSequence() 
{
  BaseTokenStreamTestCase::assertAnalyzesTo(
      a, L"ğŸ‘©â€â¤ï¸â€ğŸ‘©", std::deque<wstring>{L"ğŸ‘©â€â¤ï¸â€ğŸ‘©"},
      std::deque<wstring>{L"<EMOJI>"});
}

void TestICUTokenizer::testEmojiSequenceWithModifier() 
{
  BaseTokenStreamTestCase::assertAnalyzesTo(
      a, L"ğŸ‘¨ğŸ¼â€âš•ï¸", std::deque<wstring>{L"ğŸ‘¨ğŸ¼â€âš•ï¸"},
      std::deque<wstring>{L"<EMOJI>"});
}

void TestICUTokenizer::testEmojiRegionalIndicator() 
{
  BaseTokenStreamTestCase::assertAnalyzesTo(
      a, L"ğŸ‡ºğŸ‡¸ğŸ‡ºğŸ‡¸", std::deque<wstring>{L"ğŸ‡ºğŸ‡¸", L"ğŸ‡ºğŸ‡¸"},
      std::deque<wstring>{L"<EMOJI>", L"<EMOJI>"});
}

void TestICUTokenizer::testEmojiVariationSequence() 
{
  BaseTokenStreamTestCase::assertAnalyzesTo(a, L"#ï¸âƒ£",
                                            std::deque<wstring>{L"#ï¸âƒ£"},
                                            std::deque<wstring>{L"<EMOJI>"});
  BaseTokenStreamTestCase::assertAnalyzesTo(a, L"3ï¸âƒ£",
                                            std::deque<wstring>{L"3ï¸âƒ£"},
                                            std::deque<wstring>{L"<EMOJI>"});
}

void TestICUTokenizer::testEmojiTagSequence() 
{
  BaseTokenStreamTestCase::assertAnalyzesTo(
      a, L"ğŸ´ó §ó ¢ó ¥ó ®ó §ó ¿",
      std::deque<wstring>{L"ğŸ´ó §ó ¢ó ¥ó ®ó §ó ¿"},
      std::deque<wstring>{L"<EMOJI>"});
}

void TestICUTokenizer::testEmojiTokenization() 
{
  // simple emoji around latin
  BaseTokenStreamTestCase::assertAnalyzesTo(
      a, L"pooğŸ’©poo", std::deque<wstring>{L"poo", L"ğŸ’©", L"poo"},
      std::deque<wstring>{L"<ALPHANUM>", L"<EMOJI>", L"<ALPHANUM>"});
  // simple emoji around non-latin
  BaseTokenStreamTestCase::assertAnalyzesTo(
      a, L"ğŸ’©ä¸­åœ‹ğŸ’©", std::deque<wstring>{L"ğŸ’©", L"ä¸­", L"åœ‹", L"ğŸ’©"},
      std::deque<wstring>{L"<EMOJI>", L"<IDEOGRAPHIC>", L"<IDEOGRAPHIC>",
                           L"<EMOJI>"});
}

void TestICUTokenizer::testRandomStrings() 
{
  checkRandomData(random(), a, 1000 * RANDOM_MULTIPLIER);
}

void TestICUTokenizer::testRandomHugeStrings() 
{
  shared_ptr<Random> random = TestICUTokenizer::random();
  checkRandomData(random, a, 100 * RANDOM_MULTIPLIER, 8192);
}

void TestICUTokenizer::testTokenAttributes() 
{
  // C++ NOTE: The following 'try with resources' block is replaced by its C++
  // equivalent: ORIGINAL LINE: try (org.apache.lucene.analysis.TokenStream ts =
  // a.tokenStream("dummy", "This is a test"))
  {
    org::apache::lucene::analysis::TokenStream ts =
        a->tokenStream(L"dummy", L"This is a test");
    shared_ptr<ScriptAttribute> scriptAtt =
        ts->addAttribute(ScriptAttribute::typeid);
    ts->reset();
    while (ts->incrementToken()) {
      assertEquals(UScript::LATIN, scriptAtt->getCode());
      assertEquals(UScript::getName(UScript::LATIN), scriptAtt->getName());
      assertEquals(UScript::getShortName(UScript::LATIN),
                   scriptAtt->getShortName());
      assertTrue(ts->reflectAsString(false).find(L"script=Latin") !=
                 wstring::npos);
    }
    ts->end();
  }
}

void TestICUTokenizer::testICUConcurrency() 
{
  int numThreads = 8;
  shared_ptr<CountDownLatch> *const startingGun =
      make_shared<CountDownLatch>(1);
  std::deque<std::shared_ptr<Thread>> threads(numThreads);
  for (int i = 0; i < threads.size(); i++) {
    threads[i] = make_shared<ThreadAnonymousInnerClass>(shared_from_this(),
                                                        startingGun, i);
    threads[i]->start();
  }
  startingGun->countDown();
  for (int i = 0; i < threads.size(); i++) {
    threads[i]->join();
  }
}

TestICUTokenizer::ThreadAnonymousInnerClass::ThreadAnonymousInnerClass(
    shared_ptr<TestICUTokenizer> outerInstance,
    shared_ptr<CountDownLatch> startingGun, int i)
{
  this->outerInstance = outerInstance;
  this->startingGun = startingGun;
  this->i = i;
}

void TestICUTokenizer::ThreadAnonymousInnerClass::run()
{
  try {
    startingGun->await();
    int64_t tokenCount = 0;
    const wstring contents = L"è‹± à¹€à¸šà¸µà¸¢à¸£à¹Œ ãƒ“ãƒ¼ãƒ« à»€àºšàº abc";
    for (int i = 0; i < 1000; i++) {
      // C++ NOTE: The following 'try with resources' block is replaced by its
      // C++ equivalent: ORIGINAL LINE: try (org.apache.lucene.analysis.Tokenizer
      // tokenizer = new ICUTokenizer())
      {
        org::apache::lucene::analysis::Tokenizer tokenizer = ICUTokenizer();
        tokenizer->setReader(make_shared<StringReader>(contents));
        tokenizer->reset();
        while (tokenizer->incrementToken()) {
          tokenCount++;
        }
        tokenizer->end();
      }
    }
    if (VERBOSE) {
      wcout << tokenCount << endl;
    }
  } catch (const runtime_error &e) {
    throw runtime_error(e);
  }
}
} // namespace org::apache::lucene::analysis::icu::segmentation