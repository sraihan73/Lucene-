using namespace std;

#include "TestICUTokenizerCJK.h"
#include "../../../../../../../java/org/apache/lucene/analysis/icu/segmentation/DefaultICUTokenizerConfig.h"
#include "../../../../../../../java/org/apache/lucene/analysis/icu/segmentation/ICUTokenizer.h"

namespace org::apache::lucene::analysis::icu::segmentation
{
using Analyzer = org::apache::lucene::analysis::Analyzer;
using BaseTokenStreamTestCase =
    org::apache::lucene::analysis::BaseTokenStreamTestCase;
using org::apache::lucene::util::LuceneTestCase::AwaitsFix;

void TestICUTokenizerCJK::setUp() 
{
  BaseTokenStreamTestCase::setUp();
  a = make_shared<AnalyzerAnonymousInnerClass>(shared_from_this());
}

TestICUTokenizerCJK::AnalyzerAnonymousInnerClass::AnalyzerAnonymousInnerClass(
    shared_ptr<TestICUTokenizerCJK> outerInstance)
{
  this->outerInstance = outerInstance;
}

shared_ptr<Analyzer::TokenStreamComponents>
TestICUTokenizerCJK::AnalyzerAnonymousInnerClass::createComponents(
    const wstring &fieldName)
{
  return make_shared<Analyzer::TokenStreamComponents>(make_shared<ICUTokenizer>(
      BaseTokenStreamTestCase::newAttributeFactory(),
      make_shared<DefaultICUTokenizerConfig>(true, true)));
}

void TestICUTokenizerCJK::tearDown() 
{
  delete a;
  BaseTokenStreamTestCase::tearDown();
}

void TestICUTokenizerCJK::testSimpleChinese() 
{
  assertAnalyzesTo(
      a, L"æˆ‘è´­ä¹°äº†é“å…·å’Œæœè£…ã€‚",
      std::deque<wstring>{L"æˆ‘", L"è´­ä¹°", L"äº†", L"é“å…·", L"å’Œ", L"æœè£…"});
}

void TestICUTokenizerCJK::testTraditionalChinese() 
{
  assertAnalyzesTo(
      a, L"æˆ‘è³¼è²·äº†é“å…·å’Œæœè£ã€‚",
      std::deque<wstring>{L"æˆ‘", L"è³¼è²·", L"äº†", L"é“å…·", L"å’Œ", L"æœè£"});
  assertAnalyzesTo(a, L"å®šç¾©åˆ‡åˆ†å­—ä¸²çš„åŸºæœ¬å–®ä½æ˜¯è¨‚å®šåˆ†è©æ¨™æº–çš„é¦–è¦å·¥ä½œ",
                   std::deque<wstring>{L"å®šç¾©", L"åˆ‡", L"åˆ†", L"å­—ä¸²", L"çš„",
                                        L"åŸºæœ¬", L"å–®ä½", L"æ˜¯", L"è¨‚å®š",
                                        L"åˆ†è©", L"æ¨™æº–", L"çš„", L"é¦–è¦",
                                        L"å·¥ä½œ"});
}

void TestICUTokenizerCJK::testChineseNumerics() 
{
  assertAnalyzesTo(a, L"ï¼™ï¼”ï¼˜ï¼“", std::deque<wstring>{L"ï¼™ï¼”ï¼˜ï¼“"});
  assertAnalyzesTo(a, L"é™¢å…§åˆ†æ©Ÿï¼™ï¼”ï¼˜ï¼“ã€‚",
                   std::deque<wstring>{L"é™¢", L"å…§", L"åˆ†æ©Ÿ", L"ï¼™ï¼”ï¼˜ï¼“"});
  assertAnalyzesTo(a, L"é™¢å…§åˆ†æ©Ÿ9483ã€‚",
                   std::deque<wstring>{L"é™¢", L"å…§", L"åˆ†æ©Ÿ", L"9483"});
}

void TestICUTokenizerCJK::testSimpleJapanese() 
{
  assertAnalyzesTo(a, L"ãã‚Œã¯ã¾ã å®Ÿé¨“æ®µéšã«ã‚ã‚Šã¾ã™",
                   std::deque<wstring>{L"ãã‚Œ", L"ã¯", L"ã¾ã ", L"å®Ÿé¨“",
                                        L"æ®µéš", L"ã«", L"ã‚ã‚Š", L"ã¾ã™"});
}

void TestICUTokenizerCJK::testSimpleJapaneseWithEmoji() 
{
  assertAnalyzesTo(a, L"ãã‚Œã¯ã¾ã å®Ÿé¨“æ®µéšã«ã‚ã‚Šã¾ã™ğŸ’©",
                   std::deque<wstring>{L"ãã‚Œ", L"ã¯", L"ã¾ã ", L"å®Ÿé¨“",
                                        L"æ®µéš", L"ã«", L"ã‚ã‚Š", L"ã¾ã™",
                                        L"ğŸ’©"});
}

void TestICUTokenizerCJK::testJapaneseTypes() 
{
  assertAnalyzesTo(a, L"ä»®åé£ã„ ã‚«ã‚¿ã‚«ãƒŠ",
                   std::deque<wstring>{L"ä»®åé£ã„", L"ã‚«ã‚¿ã‚«ãƒŠ"},
                   std::deque<wstring>{L"<IDEOGRAPHIC>", L"<IDEOGRAPHIC>"});
}

void TestICUTokenizerCJK::testKorean() 
{
  // Korean words
  assertAnalyzesTo(a, L"ì•ˆë…•í•˜ì„¸ìš” í•œê¸€ì…ë‹ˆë‹¤",
                   std::deque<wstring>{L"ì•ˆë…•í•˜ì„¸ìš”", L"í•œê¸€ì…ë‹ˆë‹¤"});
}

void TestICUTokenizerCJK::testKoreanTypes() 
{
  assertAnalyzesTo(a, L"í›ˆë¯¼ì •ìŒ", std::deque<wstring>{L"í›ˆë¯¼ì •ìŒ"},
                   std::deque<wstring>{L"<HANGUL>"});
}

void TestICUTokenizerCJK::testRandomStrings() 
{
  checkRandomData(random(), a, 10000 * RANDOM_MULTIPLIER);
}

void TestICUTokenizerCJK::testRandomHugeStrings() 
{
  shared_ptr<Random> random = TestICUTokenizerCJK::random();
  checkRandomData(random, a, 100 * RANDOM_MULTIPLIER, 8192);
}
} // namespace org::apache::lucene::analysis::icu::segmentation